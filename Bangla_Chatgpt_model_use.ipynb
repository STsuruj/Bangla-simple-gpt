{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip bangla_chat_model.zip -d /content/bangla_chat_model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QUcbOK7IZjW",
        "outputId": "d4ac7e95-b21f-4e49-87a4-b73681e1d5c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  bangla_chat_model.zip\n",
            "   creating: /content/bangla_chat_model/bangla_chat_model/\n",
            "  inflating: /content/bangla_chat_model/bangla_chat_model/config.json  \n",
            "  inflating: /content/bangla_chat_model/bangla_chat_model/tokenizer.json  \n",
            "  inflating: /content/bangla_chat_model/bangla_chat_model/tokenizer_config.json  \n",
            "  inflating: /content/bangla_chat_model/bangla_chat_model/special_tokens_map.json  \n",
            "  inflating: /content/bangla_chat_model/bangla_chat_model/generation_config.json  \n",
            "  inflating: /content/bangla_chat_model/bangla_chat_model/model.safetensors  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f5kh_TTEHOu_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_path = \"/content/bangla_chat_model/bangla_chat_model\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer & model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "IW9EXpwHHUYW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move to GPU (‡¶Ø‡¶¶‡¶ø GPU available ‡¶•‡¶æ‡¶ï‡ßá)\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "-N4_iO5uHW_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 6Ô∏è‚É£ Test / Chat\n",
        "# ================================\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "def chat(prompt):\n",
        "    full_prompt = f\"User: {prompt}\\nBot:\"\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Move input_ids to model's device\n",
        "    device = model.device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    from transformers import GenerationConfig\n",
        "    gen_cfg = GenerationConfig(max_new_tokens=50, temperature=0.7, top_p=0.9, do_sample=True)\n",
        "\n",
        "    outputs = model.generate(**inputs, generation_config=gen_cfg)\n",
        "    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"ü§ñ:\", reply.split(\"Bot:\")[-1].strip())"
      ],
      "metadata": {
        "id": "2u7IELYLHa3u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "chat(\"‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶ï‡ßÄ?\")\n",
        "chat(\"‡¶Ü‡¶ú‡¶ï‡ßá‡¶∞ ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡ßü‡¶æ ‡¶ï‡ßá‡¶Æ‡¶®?\")\n",
        "chat(\"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶ï‡ßã‡¶•‡¶æ‡ßü?\")"
      ],
      "metadata": {
        "id": "nhBTUpDnHh-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0566ae8-0025-4e40-aa5d-c4587cb015d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ: ‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,‡ßç‡¶Ø‡¶á,\n",
            "ü§ñ: ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡ßü‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π, ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶¨‡¶π\n",
            "ü§ñ: ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ\n"
          ]
        }
      ]
    }
  ]
}